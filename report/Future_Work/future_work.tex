% \newpage
\section{Future Work - Multiple authors}
Most of the project members have already stated the future work in their respective sections of the report. Some of the members have also opted for adding additional information about future work in this section. After those additions future work for the MDH@HOME and RoboCup@HOME is presented.



\vspace{0.5cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent\textbf{Alexander Karlsson}
    \begin{displayquote}
        Mapping the environment, while being of lower priority, may be needed once the Butler is able to successfully locate and grasp a cup. It may be advantageous for the robot to map out its area of work in order to adapt to changes more smoothly however, the map could be made using an external source. If the Butler is to update the map the ROS package Gmapping (copy paste some info here) that was implemented in the Unicorn project may be used. This would require implementation of a converter\footnote{\url{http://wiki.ros.org/pointcloud_to_laserscan}} for the point-cloud generated by the Orbbec, to the laser-scan format that is expected by Gmapping. Another alternative is to implement RGB-D SLAM by utilizing the package RTAB-map\footnote{\url{http://wiki.ros.org/rtabmap_ros}}.
    \end{displayquote}
\noindent\textbf{Billy Lindgren}
    \begin{displayquote}
        At the moment only 1 battery can be charged at a time, while 2 are being utilised, 1 for the platform and 1 for the computers and sensors. Since this has had a low priority, nothing has been investigated thoroughly, but one idea is to make a PCB that parallel connects the batteries and have 2 outputs: one for the platform and on for the computers. If it is done in this manner the already existing charger that comes with the platform can be utilised instead of buying a new one and change how the charging should be performed. 
    \end{displayquote}

% \noindent\textbf{Jane Doe} %
%     \begin{displayquote}
%         \blindtext 
%     \end{displayquote}
\vspace{0.4cm}
This part of the section presents the specific work for MDH@HOME and RoboCup@HOME for moving forward with the project. \\
    
\subsection*{MDH@HOME - Henrik Falk} %
    
        First and foremost, the assembly of the robot has not been completed. This is the most important part of the future work. At the moment, most of the systems have been tested individually but a complete test system with logging is desired.\\
        \indent As mentioned previously in this section, map acquisition can be done with the RGB-D camera that is present and localisation is easily implemented with the work from the UNICORN project.\\
        \indent Because of the placement of the RGB-D camera for object recognition, a movable head needs to be developed or purchased. This is needed so the robot can track the object(s) it is supposed to grab.\\
        \indent The motor selection this year was a emergency solution due to external factors. Therefore, a motor upgrade is suggested with the items we have listed in this report. The motors have a long delivery time.\\
        \indent The gripper was not prioritised because of the other work and in addition to build it to a working state, it needs integration with the robot.\\
        \indent The ROS CAN implementation needs some additional care since it is write only at the moment.
    
    
\subsection*{RoboCup@HOME - Henrik Falk} %
    
        Successful qualification with respect to what is stated in Section \ref{qualification} with the desired technical abilities in Section \ref{techab} is the roadmap of completing the robot for RoboCup@HOME. Most of these are covered in this years iteration of MDH@HOME and in the future work of the project. Specific challenges for the competition which has not been explored this year is Lidar, Human Detection and, Human-Robot interaction.\\
        \indent Lidar has been utilised in the UNICORN project and can be used in this project. It shall be said that how they are utilised might differ and needs to be investigated and analysed if implementation is needed.\\
        \indent Human detection/classification is needed and with YOLO needs to be added to the weight file or trained. With consideration to the training done within the Butler project this year, human detection needs a lot of computational time if training is needed.\\
        \indent Within the competition the robot interacts with the human seamlessly, for this speech synthesis and recognition is needed. There are several methods for this in ROS, offline and online.

